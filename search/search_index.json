{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"actuated","text":"<p>Actuated brings blazingly fast, secure builds to self-hosted CI runners.</p>"},{"location":"#building-containers-on-self-hosted-runners-is-slow-and-insecure","title":"Building containers on self-hosted runners is slow and insecure","text":"<p>Most solutions that use containers for running Docker or Kubernetes in CI have very poor security boundaries. They require either privileged containers (root on the host), a shared Docker socket (root on the host), third-party tools which don't integrate well and still require root to mount folders, or user namespaces which come with their own limitations. The chances are, if you use Docker or K8s in your CI, and run with: actions-runner-controller, Jenkins, or GitLab, then you may be compromising on security or user experience.</p>"},{"location":"#management-is-a-nightmare","title":"Management is a nightmare","text":"<p>Self-hosted CI runners are continually out of date, and require fine-tuning to get all the right packages in place and Kernel modules to build containers and cloud-native software. You'll also have to spend extra time making sure builds don't conflict, and that they can't cause side effects to system-level packages. What if you need two different version of some software?</p> <p>If you haven't felt this pain yet, then perhaps you're blissfully unaware or are not updating your packages?</p> <p>Are you running privileged containers for CI in your organisation? Are you sharing a Docker Socket (just as bad!)? Are you running Docker in Docker (DIND)? \ud83d\ude48</p>"},{"location":"#self-managed-runners-are-inefficient-and-overprovisioned","title":"Self-managed runners are inefficient and overprovisioned","text":"<p>Self-hosted runners are typically over-provisioned meaning you're spending too much money.</p> <p>Why are they over-provisioned? Because you never know how many jobs you'll have to run, so you have to make them bigger, or have too many hosts available.</p> <p>Why are they inefficient?</p> <p>By default, the self-hosted runner will only schedule one job per host at a time, because GitHub has no knowledge of the capacity of your machines. So each and every build you run could consume all the resources on the host. The second reason is that builds often conflict with one another causing side effects that only happen in CI and are really difficult to track down and reproduce.</p> <p>Actuated uses VMs to slice up the whole machine, and can run many builds in parallel. The net effect is that your build queue will get cleared down much more quickly.</p>"},{"location":"#hands-free-vm-level-isolation","title":"Hands-free, VM-level isolation","text":"<p>Actuated provides a fast-booting microVM which can run Docker, Kubernetes and anything else you need, with full root on the VM, and no access to the host. Each environment is created just in time to take a build, and is removed immediately after.</p> <p>Boot time is usually ~1-2 seconds for the VM, that extra second is because we start Docker as part of the boot-up process.</p> <p>What does \"actuated\" mean?</p> <p>Something that activates or impels itself; specifically (a machine, device, etc.) that causes itself to begin operating automatically, self-activating.</p> <p>We maintain a VM image that is updated regularly through an automated build, so you don't have to install SDKs, runtimes or language packs on your build machines.</p> <p>Just enable automated updates on your server then install the actuated agent. We'll do the rest including managing efficient allocation across your fleet of servers, and updating the CI image.</p> <p>And actuated will run your jobs efficiently across a fleet of hosts, or a single machine. They each need to be either bare-metal hosts (think: AWS Metal / Graviton, Equinix Metal, etc), or support nested virtualization (a feature available on GCP and DigitalOcean)</p>"},{"location":"#what-people-are-saying","title":"What people are saying","text":"<ul> <li> <p>\"We've been piloting Actuated recently. It only took 30s create 5x isolated VMs, run the jobs and tear them down again inside our on-prem environment (no Docker socket mounting shenanigans)! Pretty impressive stuff.\"</p> <p>Addison van den Hoeven - DevOps Lead, Riskfuel</p> </li> <li> <p>\"This is great, perfect for jobs that take forever on normal GitHub runners. I love what Alex is doing here.\"</p> <p>Richard Case, Principal Engineer, SUSE</p> </li> <li> <p>\"Thank you. I think actuated is amazing.\"</p> <p>Alan Sill, NSF Cloud and Autonomic Computing (CAC) Industry-University Cooperative Research Center</p> </li> <li> <p>\"Nice work, security aspects alone with shared/stale envs on self-hosted runners.\"</p> <p>Matt Johnson, Palo Alto Networks</p> </li> <li> <p>\"Is there a way to pay github for runners that suck less?\"</p> <p>Darren Shepherd, Acorn Labs</p> </li> <li> <p>\"Excited to try out actuated! We use custom actions runners and I think there's something here \ud83d\udd25\"</p> <p>Nick Gerace, System Initiative</p> </li> <li> <p>It is awesome to see the work of Alex Ellis with Firecracker VMs. They are provisioning and running GitHub Actions in isolated VMs in seconds (vs minutes).\"</p> <p>Rinat Abdullin, ML &amp; Innovation at Trustbit</p> </li> <li> <p>\"This is awesome!\" (After reducing Parca build time from 33.5 minutes to 1 minute 26s)</p> <p>Frederic Branczyk, Co-founder, Polar Signals</p> </li> </ul>"},{"location":"#watch-a-live-demo","title":"Watch a live demo","text":"<p>Alex shows you how actuated uses an isolated, immutable microVM to run K3s inside of a GitHub Action, followed by a matrix build that causes 5 VMs to be launched. You'll see how quick and easy it is to enable actuated, and how it can buffer and queue up jobs, when there's no remaining capacity in your fleet of agents.</p> <p>You can also watch a webinar that Alex recorded with Richard Case from Weaveworks on how microVMs compare to containers and legacy VMs, you'll see Alex's demo at: 1:13:19.</p>"},{"location":"#conceptual-overview","title":"Conceptual overview","text":"<p>Actuated will schedule builds across your fleet of agents, packing them in densely, without overloading the host. Each microVM will run just one build before being destroyed to ensure a clean, isolated build. </p> <p>Learn more in the FAQ</p>"},{"location":"#get-started","title":"Get started","text":"<ul> <li>Start a subscription or book a call to find out more</li> <li>Read the FAQ</li> <li>Enable actuated for an existing repository</li> <li>Read more in the announcement: Blazing fast CI with MicroVMs</li> </ul>"},{"location":"#comparison","title":"Comparison","text":"<p>Feel free to book a call with us if you'd like to understand this comparison in more detail.</p> Solution Isolated VM Speed Efficient spread of jobs Safely build public repos? ARM64 support Maintenance required Cost Hosted runners Poor None Free minutes in plan <code>1</code> Per build minute actuated Bare-metal Yes Very little Fixed monthly cost Standard self-hosted runners Good DIY Manual setup and updates OSS plus management costs actions-runtime-controller Varies <code>2</code> DIY Very involved OSS plus management costs <p><code>1</code> actions-runtime-controller requires use of separate build tools such as Kaniko, which break the developer experience of using <code>docker</code> or <code>docker-compose</code>. If Docker in Docker (DinD) is used, then there is a severe performance penalty and security risk.</p> <p><code>2</code> Builds on public GitHub repositories are free with the standard hosted runners, however private repositories require billing information, after the initial included minutes are consumed.</p> <p>You can only get VM-level isolation from either GitHub hosted runners or Actuated. Standard self-hosted runners have no isolation between builds and actions-runtime-controller requires either a Docker socket to be mounted or Docker In Docker (a privileged container) to build and run containers.</p>"},{"location":"#got-questions-comments-or-suggestions","title":"Got questions, comments or suggestions?","text":"<p>actuated is trademark of OpenFaaS Ltd.</p> <p>You can contact the team working on actuated via email at: contact@openfaas.com</p> <p>Follow @selfactuated on Twitter for updates and announcements</p>"},{"location":"contact/","title":"Contact us","text":""},{"location":"contact/#contact-us","title":"Contact us","text":"<p>Would you like to contact us about actuated for your team or oranisation?</p> <p>Fill out this form, and we'll get in touch shortly after with next steps.</p> <p>Actuated \u2122 is a trademark of OpenFaaS Ltd.</p>"},{"location":"contact/#keeping-in-touch","title":"Keeping in touch","text":"<ul> <li>Follow us on Twitter - @selfactuated</li> <li>GitHub - github.com/self-actuated</li> <li>Customer Slack - self-actuated.slack.com</li> </ul>"},{"location":"contact/#anything-else","title":"Anything else?","text":"<p>Looking for technical details about actuated? Try the FAQ.</p> <p>Are you running into a problem? Try the troubleshooting guide</p>"},{"location":"expose-agent/","title":"Expose agent","text":""},{"location":"expose-agent/#expose-the-agents-api-over-https","title":"Expose the agent's API over HTTPS","text":"<p>The actuated agent serves HTTP, and must be accessible by the actuated control plane.</p> <p>We expect most of our pilot customers to be using hosts with public IP addresses, and the combination of an API token plus TLS is a battle tested combination.</p> <p>For anyone running with private hosts, OpenFaaS Ltd's inlets product can be used to get incoming traffic over a secure tunnel</p>"},{"location":"expose-agent/#for-a-host-on-a-public-cloud","title":"For a host on a public cloud","text":"<p>If you're running the agent on a host with a public IP, you can use the built-in TLS mechanism in the actuated agent to receive a certificate from Let's Encrypt, valid for 90 days. The certificate will be renewed by the actuated agent, so there are no additional administration tasks required.</p> <p></p> <p>Pictured: Accessing the agent's endpoint built-in TLS and Let's Encrypt</p> <p>Determine the public IP of your instance:</p> <pre><code># curl -s http://checkip.amazonaws.com\n\n141.73.80.100\n</code></pre> <p>Now imagine that your sub-domain is <code>agent.example.com</code>, you need to create a DNS A record of <code>agent.example.com=141.73.80.100</code>, changing both the sub-domain and IP to your own.</p> <p>Once created, edit the start.sh file on the agent and add two flags:</p> <pre><code>--letsencrypt-domain agent.example.com \\\n--letsencrypt-email webmaster@agent.example.com\n</code></pre> <p>Your agent's endpoint URL is going to be: <code>https://agent.example.com</code> on port 443</p>"},{"location":"expose-agent/#private-hosts-on-premises-behind-nat-or-at-home","title":"Private hosts - on-premises, behind NAT or at home","text":"<p>You'll need a way to expose the client to the Internet, which includes HTTPS encryption and a sufficient amount of connections/traffic per minute.</p> <p>Inlets provides a quick and secure solution here. It is available on a monthly subscription, bear in mind that the \"Personal\" plan is not for this kind of commercial use.</p> <p></p> <p>Pictured: Accessing the agent's private endpoint using an inlets-pro tunnel</p> <p>Reach out to us if you'd like us to host a tunnel server for you, alternatively, you can follow the instructions below to set up your own.</p> <p>The inletsctl tool will create a HTTPS tunnel server with you on your favourite cloud with a HTTPS certificate obtained from Let's Encrypt.</p> <p>If you have just the one Actuated Agent:</p> <pre><code>export AGENT_DOMAIN=agent1.example.com\nexport LE_EMAIL=webmaster@agent1.example.com\n\narkade get inletsctl\nsudo mv $HOME/.arkade/bin/inletsctl /usr/local/bin/\n\ninletsctl create \\\n--provider digitalocean \\\n--region lon1 \\\n--token-file $HOME/do-token \\\n--letsencrypt-email $LE_EMAIL \\\n--letsencrypt-domain $AGENT_DOMAIN\n</code></pre> <p>Then note down the tunnel's wss:// URL and token.</p> <p>Then run a HTTPS client to expose your agent:</p> <pre><code>inlets-pro http client \\\n--url $WSS_URL \\\n--token $TOKEN \\\n--upstream http://127.0.0.1:8081\n</code></pre> <p>For two or more Actuated Servers:</p> <pre><code>export AGENT_DOMAIN1=agent1.example.com\nexport AGENT_DOMAIN2=agent2.example.com\nexport LE_EMAIL=webmaster@agent1.example.com\n\narkade get inletsctl\nsudo mv $HOME/.arkade/bin/inletsctl /usr/local/bin/\n\ninletsctl create \\\n--provider digitalocean \\\n--region lon1 \\\n--token-file $HOME/do-token \\\n--letsencrypt-email $LE_EMAIL \\\n--letsencrypt-domain $AGENT_DOMAIN1 \\\n--letsencrypt-domain $AGENT_DOMAIN2\n</code></pre> <p>Then note down the tunnel's wss:// URL and token.</p> <p>Then run a HTTPS client to expose your agent, using the unique agent domain, run the inlets-pro client on the Actuated Servers:</p> <pre><code>export AGENT_DOMAIN1=agent1.example.com\ninlets-pro http client \\\n--url $WSS_URL \\\n--token $TOKEN \\\n--upstream $AGENT1_DOMAIN=http://127.0.0.1:8081\n</code></pre> <pre><code>export AGENT_DOMAIN2=agent2.example.com\ninlets-pro http client \\\n--url $WSS_URL \\\n--token $TOKEN \\\n--upstream $AGENT1_DOMAIN=http://127.0.0.1:8081\n</code></pre> <p>You can generate a systemd service (so that inlets restarts upon disconnection, and reboot) by adding <code>--generate=systemd &gt; inlets.service</code> and running:</p> <pre><code>sudo cp inlets.service /etc/systemd/system/\nsudo systemctl daemon-reload\nsudo systemctl enable inlets.service\nsudo systemctl start inlets\n\n# Check status with:\nsudo systemctl status inlets\n</code></pre> <p>Your agent's endpoint URL is going to be: <code>https://$AGENT_DOMAIN</code>.</p>"},{"location":"expose-agent/#preventing-the-runner-from-accessing-your-local-network","title":"Preventing the runner from accessing your local network","text":"<p>Network segmentation</p> <p>Proper network segmentation of hosts running the actuated agent is required. This is to prevent runners from making outbound connections to other hosts on your local network. We will not accept any responsibility for your configuration.</p> <p>If hardware isolation is not available, iptables rules may provide an alternative for isolating the runners from your network.</p> <p>Imagine you were using a LAN range of <code>192.168.0.0/24</code>, with a router of <code>192.168.0.1</code>, then the following probes and tests show that the runner cannot access the host 192.168.0.101, and that nmap's scan will come up dry.</p> <p>We add a rule to allow access to the router, but reject packets going via TCP or UDP to any other hosts on the network.</p> <pre><code>sudo iptables --insert CNI-ADMIN \\\n--destination  192.168.0.1 --jump ACCEPT\nsudo iptables --insert CNI-ADMIN \\\n--destination  192.168.0.0/24 --jump REJECT -p tcp  --reject-with tcp-reset\nsudo iptables --insert CNI-ADMIN \\\n--destination  192.168.0.0/24 --jump REJECT -p udp --reject-with icmp-port-unreachable\n</code></pre> <p>You can test the efficacy of these rules by running nmap, mtr, ping and any other probing utilities within a GitHub workflow.</p> <pre><code>name: CI\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n- main\n\njobs:\nspecs:\nname: specs\nruns-on: actuated\nsteps:\n- uses: actions/checkout@v1\n- name: addr\nrun: ip addr\n- name: route\nrun: ip route\n- name: pkgs\nrun: |\nsudo apt-get update &amp;&amp; \\\nsudo apt-get install traceroute mtr nmap netcat -qy\n- name: traceroute\nrun: traceroute  192.168.0.101\n- name: Connect to ssh\nrun: echo | nc  192.168.0.101 22\n- name: mtr\nrun: mtr -rw  -c 1  192.168.0.101\n- name: nmap for SSH\nrun: nmap -p 22  192.168.0.0/24\n- name: Ping router\nrun: |\nping -c 1  192.168.0.1\n- name: Ping 101\nrun: |\nping -c 1  192.168.0.101\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#how-does-it-work","title":"How does it work?","text":"<p>Actuated has three main parts:</p> <ol> <li>an agent which knows how to run VMs, you install this on your hosts</li> <li>a VM image and Kernel that we build which has everything required for Docker, KinD and K3s</li> <li>a multi-tenant control plane that we host, which tells your agents to start VMs and register a runner on your GitHub organisation</li> </ol> <p>The multi-tenant control plane is run and operated by OpenFaaS Ltd as a SaaS.</p> <p></p> <p>The conceptual overview showing how a MicroVM is requested by the control plane.</p> <p>MicroVMs are only started when needed, and are registered with GitHub by the official GitHub Actions runner, using a short-lived registration token. The token is been encrypted with the public key of the agent. This ensures no other agent could use the token to bootstrap a token to the wrong organisation.</p> <p>Learn more: Self-hosted GitHub Actions API</p>"},{"location":"faq/#glossary","title":"Glossary","text":"<ul> <li><code>MicroVM</code> - a lightweight, single-use VM that is created by the Actuated Agent, and is destroyed after the build is complete. Common examples include firecracker by AWS and Cloud Hypervisor</li> <li><code>Guest Kernel</code> - a Linux kernel that is used together with a Root filesystem to boot a MicroVM and run your CI workloads</li> <li><code>Root filesystem</code> - an immutable image maintained by the actuated team containing all necessary software to perform a build</li> <li><code>Actuated</code> ('Control Plane') - a multi-tenant SaaS run by the actuated team responsible for scheduling MicroVMs to the Actuated Agent </li> <li><code>Actuated Agent</code> - the software component installed on your Server which runs a MicroVM when instructed by Actuated</li> <li><code>Actuated Server</code> ('Server') - a server on which the Actuated Agent has been installed, where your builds will execute.</li> </ul>"},{"location":"faq/#how-does-actuated-compare-to-a-self-hosted-runner","title":"How does actuated compare to a self-hosted runner?","text":"<p>A self-hosted runner is a machine on which you've installed and registered the a GitHub runner.</p> <p>Quite often these machines suffer from some, if not all of the following issues:</p> <ul> <li>They require several hours to get all the required packages correctly installed to mirror a hosted runner</li> <li>You never update them out of fear of wasting time or breaking something which is working, meaning your supply chain is at risk</li> <li>Builds clash, if you're building a container image, or running a KinD cluster, names will clash, dirty state will be left over</li> </ul> <p>We've heard in user interviews that the final point of dirty state can cause engineers to waste several days of effort chasing down problems.</p> <p>Actuated uses a one-shot VM that is destroyed immediately after a build is completed.</p>"},{"location":"faq/#who-is-actuated-for","title":"Who is actuated for?","text":"<p>actuated is primarily for software engineering teams who are currently using GitHub Actions. A GitHub organisation is required for installation, and runners are attached to individual repositories as required, to execute builds.</p>"},{"location":"faq/#what-kind-of-machines-do-i-need-for-the-agent","title":"What kind of machines do I need for the agent?","text":"<p>You'll need either: a bare-metal host (your own, AWS i3.metal or Equinix Metal), or a VM that supports nested virtualisation such as those provided by GCP, DigitalOcean and Azure.</p>"},{"location":"faq/#when-will-jenkins-gitlab-ci-bitbucket-pipeline-runners-drone-or-azure-devops-be-supported","title":"When will Jenkins, GitLab CI, BitBucket Pipeline Runners, Drone or Azure DevOps be supported?","text":"<p>For the pilot phase, we're targeting GitHub Actions because it has fine-grained access controls and the ability to schedule exactly one build to a runner. Most other CI systems expect self-hosted runners to perform many builds, and we believe that to be an anti-pattern. We'll offer advice to teams accepted into the pilot who wish to evaluate GitHub Actions or migrate away from another solution.</p> <p>That said, if you're using these tools within your organisation, and face similar issues or concerns, we'd like to hear from you. And we have a proof of concept that works with GitLab CI, so feel free to reach out to us if you feel actuated would be a good fit for your team.</p> <p>Feel free to contact us at: contact@openfaas.com</p>"},{"location":"faq/#is-github-enterprise-supported","title":"Is GitHub Enterprise supported?","text":"<p>GitHub.com's Pro, Team and Enterprise Cloud plans are supported.</p> <p>GitHub Enterprise Server (GHES) is a self-hosted version of GitHub and may require additional configuration. Please reach out to us if you're interested in using actuated with your installation of GHES.</p>"},{"location":"faq/#what-kind-of-access-is-required-to-my-github-organisation","title":"What kind of access is required to my GitHub Organisation?","text":"<p>GitHub Apps provide fine-grained privileges, access control, and event data.</p> <p>Actuated integrates with GitHub using a GitHub App.</p> <p>The actuated GitHub App will request:</p> <ul> <li>Administrative access to add/remove GitHub Actions Runners to individual repositories</li> <li>Events via webhook for Workflow Runs and Workflow Jobs</li> </ul> <p>Did you know? The actuated service does not need any access to your code or private or public repositories.</p>"},{"location":"faq/#can-githubs-self-hosted-runner-be-used-on-public-repos","title":"Can GitHub's self-hosted runner be used on public repos?","text":"<p>The GitHub team recommends only running their self-hosted runners on private repositories.</p> <p>Why?</p> <p>I took some time to ask one of the engineers on the GitHub Actions team.</p> <p>With the standard self-hosted runner, a bad actor could compromise the system or install malware leaving side-effects for future builds.</p> <p>He replied that it's difficult for maintainers to secure their repos and workflows, and that bad actors could compromise a runner host due to the way they run multiple jobs, and are not a fresh environment for each build. It may even be because a bad actor could scan the local network of the runner and attempt to gain access to other systems.</p> <p>If you're wondering whether containers and Pods are a suitable isolation level, we would recommend against this since it usually involves one of either: mounting a docker socket (which can lead to escalation to root on the host) or running Docker In Docker (DIND) which requires a privileged container (which can lead to escalation to root on the host).</p> <p>So, can you use actuated on a public repo?</p> <p>Our contact at GitHub stated that through VM-level isolation and an immutable VM image, the primary concerns is resolved, because there is no way to have state left over or side effects from previous builds.</p> <p>Actuated fixes the isolation problem, and prevents side-effects between builds. We also have specific iptables rules in the troubleshooting guide which will isolate your runners from the rest of the network.</p>"},{"location":"faq/#can-i-use-the-containers-feature-of-github-actions","title":"Can I use the containers feature of GitHub Actions?","text":"<p>GitHub Action's Running jobs in a container feature is supported, as is Docker, Buildx, Kubernetes, KinD, K3s, etc.</p> <p>Example of running commands with the <code>docker.io/node:16</code> image.</p> <pre><code>jobs:\nspecs:\nname: test\nruns-on: actuated\ncontainer:\nimage: docker.io/node:16\nenv:\nNODE_ENV: development\nports:\n- 3000\noptions: --cpus 1\nsteps:\n- name: Check for dockerenv file\nrun: node --version\n</code></pre>"},{"location":"faq/#how-many-builds-does-a-single-actuated-vm-run","title":"How many builds does a single actuated VM run?","text":"<p>When a VM starts up, it runs the GitHub Actions Runner ephemeral (aka one-shot) mode, so in can run at most one build. After that, the VM will be destroyed.</p> <p>See also: GitHub: ephemeral runners</p>"},{"location":"faq/#how-are-vms-scheduled","title":"How are VMs scheduled?","text":"<p>VMs are placed efficiently across your Actuated Servers using a scheduling algorithm based upon the amount of RAM reserved for the VM.</p> <p>Autoscaling of VMs is automatic. Let's say that you had 10 jobs pending, but given the RAM configuration, only enough capacity to run 8 of them? The second two would be queued until capacity one or more of those 8 jobs completed.</p> <p>If you find yourself regularly getting into a queued state, there are three potential changes to consider:</p> <ol> <li>Using Actuated Servers with more RAM</li> <li>Allocated less RAM to each job</li> <li>Adding more Actuated Servers</li> </ol> <p>The plan you select will determine how many Actuated Servers you can run, so consider 1. and 2. before 3.</p>"},{"location":"faq/#do-i-need-to-auto-scale-the-actuated-servers","title":"Do I need to auto-scale the Actuated Servers?","text":"<p>Please read the section \"How are VMs scheduled\".</p> <p>Auto-scaling Pods or VMs is a quick, painless operation that makes sense for customer traffic, which is generally unpredictable and can be very bursty.</p> <p>GitHub Actions tends to be driven by your internal development team, with a predictable pattern of activity. It's unlikely to vary massively day by day, which means autoscaling is less important than with a user-facing website.</p> <p>In addition to that, bare-metal servers can take 5-10 minutes to provision and may even include a setup fee or monthly commitment, meaning that what you're used to seeing with Kubernetes or AWS Autoscaling Groups may not translate well, or even be required for CI.</p> <p>If you are cost sensitive, you should review the options under Provision a Server section.</p> <p>Depending on your provider, you may also be able to hibernate or suspend servers on a cron schedule to save a few dollars. Actuated will hold jobs in a queue until a server is ready to take them again.</p>"},{"location":"faq/#what-do-i-need-to-change-in-my-workflows-to-use-actuated","title":"What do I need to change in my workflows to use actuated?","text":"<p>Very little, just add / set <code>runs-on: actuated</code></p>"},{"location":"faq/#is-arm64-supported","title":"Is ARM64 supported?","text":"<p>Yes, actuated is built to run on both Intel/AMD and ARM64 hosts, check your subscription plan to see if ARM64 is included. This includes a Raspberry Pi 4B, AWS Graviton, Oracle Cloud ARM instances and potentially any other ARM64 instances which support virtualisation.</p>"},{"location":"faq/#whats-in-the-vm-image-and-how-is-it-built","title":"What's in the VM image and how is it built?","text":"<p>The VM image contains similar software to the hosted runner image: <code>ubuntu-latest</code> offered by GitHub. Unfortunately, GitHub does not publish this image, so we've done our best through user-testing to reconstruct it, including all the Kernel modules required to run Kubernetes and Docker.</p> <p>The image is built automatically using GitHub Actions and is available on a container registry.</p>"},{"location":"faq/#how-easy-is-it-to-debug-a-runner","title":"How easy is it to debug a runner?","text":"<p>OpenSSH is pre-installed, but it will be inaccessible from your workstation by default.</p> <p>To connect, you can use an inlets tunnel, Wireguard VPN or Tailscale ephemeral token (remember: Tailscale is not free for your commercial use) to log into any agent.</p> <p>We also offer a SSH gateway in some of our tiers, tell us if this is important to you in your initial contact, or reach out to us via email if you're already a customer.</p> <p>See also: Debug a GitHub Action with SSH</p>"},{"location":"faq/#comparison-to-other-solutions","title":"Comparison to other solutions","text":"<p>Feel free to book a call with us if you'd like to understand this comparison in more detail.</p> Solution Isolated VM Speed Efficient spread of jobs Safely build public repos? ARM64 support Maintenance required Cost Hosted runners Poor None Free minutes in plan <code>*</code> Per build minute actuated Bare-metal Yes Very little Fixed monthly cost Standard self-hosted runners Good DIY Manual setup and updates OSS plus management costs actions-runtime-controller Varies <code>*</code> DIY Very involved OSS plus management costs <p><code>1</code> actions-runtime-controller requires use of separate build tools such as Kaniko, which break the developer experience of using <code>docker</code> or <code>docker-compose</code>. If Docker in Docker (DinD) is used, then there is a severe performance penalty and security risk.</p> <p><code>2</code> Builds on public GitHub repositories are free with the standard hosted runners, however private repositories require billing information, after the initial included minutes are consumed.</p> <p>You can only get VM-level isolation from either GitHub hosted runners or Actuated. Standard self-hosted runners have no isolation between builds and actions-runtime-controller requires either a Docker socket to be mounted or Docker In Docker (a privileged container) to build and run containers.</p>"},{"location":"faq/#how-does-actuated-compare-to-a-actions-runtime-controller-arc","title":"How does actuated compare to a actions-runtime-controller (ARC)?","text":"<p>actions-runtime-controller (ARC) describes itself as \"still in its early stage of development\". It was created by an individual developer called Yusuke Kuoka, and now receives updates from GitHub's team, after having been adopted into the actions GitHub Organisation.</p> <p>Its primary use-case is scale GitHub's self-hosted actions runner using Pods in a Kubernetes cluster. ARC is self-hosted software which means its setup and operation are complex, requiring you to create an properly configure a GitHub App along with its keys. For actuated, you only need to run a single binary on each of your runner hosts and send us an encrypted bootstrap token.</p> <p>If you're running <code>npm install</code> or <code>maven</code>, then this may be a suitable isolation boundary for you.</p> <p>The default mode for ARC is a reuseable runner, which can run many jobs, and each job could leave side-effects or poison the runner for future job runs.</p> <p>If you need to build a container, in a container, on a Kubernetes node offers little isolation or security boundary.</p> <p>What if ARC is configured to use \"rootless\" containers? With a rootless container, you lose access to \"root\" and <code>sudo</code>, both of which are essential in any kind of CI job. Actuated users get full access to root, and can run <code>docker build</code> without any tricks or losing access to <code>sudo</code>. That's the same experience you get from a hosted runner by GitHub, but it's faster because it's on your own hardware.</p> <p>You can even run minikube, KinD, K3s and OpenShift with actuated without any changes.</p> <p>ARC runs a container, so that should work on any machine with a modern Kernel, however actuated runs a VM, in order to provide proper isolation.</p> <p>That means ARC runners can run pretty much anywhere, but actuated runners need to be on a bare-metal machine, or a VM that supports nested virtualisation.</p> <p>See also: Where can I run my agents?</p>"},{"location":"faq/#doesnt-kaniko-fix-all-this-for-arc","title":"Doesn't Kaniko fix all this for ARC?","text":"<p>Kaniko, by Google is an open source project for building containers. It's usually run as a container itself, and usually will require root privileges in order to mount the various filesystems layers required.</p> <p>See also: Root user inside a container is root on the host</p> <p>If you're an ARC user and for various reasons, cannot migrate away to a more secure solution like actuated, Kaniko may be a step in the right direction. Google Cloud users could also create a dedicated node pool with gVisor enabled, for some additional isolation.</p> <p>However, it can only build containers, and still requires root, and itself is often run in Docker, so we're getting back to the same problems that actuated set out to solve.</p> <p>In addition, Kaniko cannot and will not help you to run that container that you've just built to validate it to run end to end tests, neither can it run a KinD cluster, or a Minikube cluster.</p>"},{"location":"faq/#do-we-need-to-run-my-actuated-servers-247","title":"Do we need to run my Actuated Servers 24/7?","text":"<p>Let's say that you wanted to access a single ARM64 runner to speed up your ARM builds from 33 minutes to &lt; 2 minutes like in this example.</p> <p>The two cheapest options for ARM64 hardware would be:</p> <ul> <li>Buy a Mac Mini M1, host it in your office or a co-lo with Asahi Linux installed. That's a one-time cost and will last for several years.</li> <li>Or you could rent an AWS a1.metal by the hour from AWS with very little up front cost, and pay for the time you use it.</li> </ul> <p>In both cases, we're not talking about a significant amount of money, however we are sometimes asked about whether Actuated Servers need to be running 24/7.</p> <p>The answer if that it's a trade-off between cost and convenience. We recommend running them continually, however you can turn them off when you're not using them if you think it is worth your time to do so.</p> <p>If you only needed to run ARM builds from 9-5pm, you could absolutely delete the VM and re-create it with a cron job, just make sure you restore the required files from the original registration of the agent. You may also be able to \"suspend\" or \"hibernate\" the host at a reduced cost, this depends on the hosting provider. Feel free to reach out to us if you need help with this.</p>"},{"location":"faq/#is-there-gpu-support","title":"Is there GPU support?","text":"<p>We are currently exploring dedicating a GPU to a build. So if an Actuated Server had 8x GPUs, you could run 8x GPU-based builds on that host at once, each with one GPU, or 2x jobs with 4x GPUS etc. Let us know if this is something you need when you get in touch with us.</p>"},{"location":"faq/#can-virtual-machines-be-launched-within-a-github-action","title":"Can Virtual Machines be launched within a GitHub Action?","text":"<p>It is possible to launch a Virtual Machine (VM) with KVM from within a Firecracker MicroVM.</p> <p>Use-cases may include: building and snapshotting VM images, running Packer, launching VirtualBox and Vagrant, accelerating the Android emulator, building packages for NixOS and other testing which requires KVM.</p> <p>It's disabled by default, but you can opt-in to the feature by following the steps in this article:</p> <p>How to run a KVM guest in your GitHub Actions</p> <p>At time of writing, only Intel and AMD CPUs support nested virtualisation. This may be on by default, but if not, you can enable it in the system's BIOS or out of band console.</p>"},{"location":"faq/#is-windows-or-macos-supported","title":"Is Windows or MacOS supported?","text":"<p>Linux is the only supported platform for actuated at this time on a AMD64 or ARM64 architecture. We may consider other operating systems in the future, feel free to reach out to us.</p>"},{"location":"faq/#is-actuated-free-and-open-source","title":"Is Actuated free and open-source?","text":"<p>Actuated currently uses the Firecracker project to launch MicroVMs to isolate jobs during CI. Firecracker is an open source Virtual Machine Manager used by Amazon Web Services (AWS) to run serverless-style workloads for AWS Lambda.</p> <p>Actuated is a commercial B2B product and service created and operated by OpenFaaS Ltd.</p> <p>Read the End User License Agreement (EULA)</p> <p>The website and documentation are available on GitHub and we plan to release some open source tools in the future to improve customer experience.</p>"},{"location":"faq/#is-there-a-risk-that-we-could-get-locked-in-to-actuated","title":"Is there a risk that we could get \"locked-in\" to actuated?","text":"<p>No, you can move back to either hosted runners (pay per minute from GitHub) or self-managed self-hosted runners at any time. Bear in mind that actuated solves for a certain set of issues with both of those approaches.</p>"},{"location":"faq/#why-is-the-brand-called-actuated-and-selfactuated","title":"Why is the brand called \"actuated\" and \"selfactuated\"?","text":"<p>The name of the software is actuated, in some places \"actuated\" is not available, and we liked \"selfactuated\" more than \"actuatedhq\" or \"actuatedio\" because it refers to the hybrid experience of self-hosted runners.</p>"},{"location":"faq/#privacy-policy-data-security","title":"Privacy policy &amp; data security","text":"<p>Actuated is a managed service operated by OpenFaaS Ltd, registered company number: 11076587.</p> <p>It has both a Software as a Service (SaaS) component (\"control plane\") aka (\"Actuated\") and an agent (\"Actuated Agent\"), which runs on a Server supplied by the customer (\"Customer Server\").</p>"},{"location":"faq/#data-storage","title":"Data storage","text":"<p>The control-plane of actuated collects and stores:</p> <ul> <li>Job events for the organisation where a label of \"actuated*\" is found, including:<ul> <li>Organisation name</li> <li>Repository name</li> <li>Actor name for each job</li> <li>Build name</li> <li>Build start / stop time</li> <li>Build status</li> </ul> </li> </ul> <p>The following is collected from agents:</p> <ul> <li>Agent version</li> <li>Hostname &amp; uptime</li> <li>Platform information - Operating System and architecture</li> <li>System capacity - total and available RAM &amp; CPU</li> </ul> <p>In addition, for support requests, we may need to collect the logs of the actuated agent process remotely from:</p> <ul> <li>VMs launched for jobs, stored at <code>/var/log/actuated/</code></li> </ul> <p>This information is required to operate the control plane including scheduling of VMs and for technical support.</p> <p>Upon cancelling a subscription, a customer may request that their data is deleted. In addition, they can uninstall the GitHub App from their organisation, and deactivate the GitHub OAuth application used to authenticate to the Actuated Dashboard.</p>"},{"location":"faq/#data-security-encryption","title":"Data security &amp; encryption","text":"<p>TLS is enabled on the actuated control plane, the dashboard and on each agent. The TLS certificates have not expired and and have no known issues.</p> <p>Each customer is responsible for hosting their own Servers and installing appropriate firewalls or access control.</p> <p>Each Customer Server requires a unique token which is encrypted using public key cryptography, before being shared with OpenFaaS Ltd. This token is used to authenticate the agent to the control plane.</p> <p>Traffic between the control plane and Customer Server is only made over HTTPS, using TLS encryption and API tokens. In addition, the token required for GitHub Actions is double encrypted with an RSA key pair, so that only the intended agent can decrypt and use it. These tokens are short-lived and expire after 59 minutes.</p> <p>Event data recorded from GitHub Actions is stored and used to deliver quality of service and scheduling. This data is stored on a server managed by DigitalOcean LLC in the United Kingdom. The control plane is hosted with Linode LLC in the United Kingdom.</p> <p>No data is shared with third parties.</p>"},{"location":"faq/#software-development-life-cycle","title":"Software Development Life Cycle","text":"<ul> <li>A Version Control System (VCS) is being Used - GitHub is used by all employees to store code</li> <li>Only Authorized Employees Access Version Control - multiple factor authentication (MFA) is required by all employees</li> <li>Only Authorized Employees Change Code - no changes can be pushed to production without having a pull request approval from senior management</li> <li>Production Code Changes Restricted - Only authorized employees can push orm make changes to production code</li> <li>All changes are documented through pull requests tickets and commit messages</li> <li>Vulnerability management - vulnerability management is provided by GitHub.com. Critical vulnerabilities are remediated in a timely manner</li> </ul> <p>Terminated Employee Access Revoked Within One Business Day - all access to source control management and production systems is revoked within one business day of an employee leaving the company.</p> <p>Access to corporate network, production machines, network devices, and support tools requires a unique ID. This ID is only issued to employees and is revoked upon termination.</p> <p>Policies Cover Employee Confidentiality - OpenFaaS Ltd policies require employees to keep confidential any information they learn while handling customer data.</p>"},{"location":"faq/#contact-information-available-to-customers","title":"Contact Information Available to Customers","text":"<p>OpenFaaS Ltd has provided an email address in a customer-accessible support documentation where support contact information is readily available. Users are encouraged to contact appropriate OpenFaaS Ltd if they become aware of items such as operational or security failures, incidents, system problems, concerns, or other issues/complaints.</p>"},{"location":"faq/#reliability-and-uptime","title":"Reliability and uptime","text":"<p>Authorized users have access to centralised logging endpoints, to query the logs of the Actuated agent installed on Customer Servers, ad-hoc, for the purpose of support and troubleshooting.</p> <p>Authorized users have access to alerts, dashboards and may use this data to improve the service, or to proactively contact customers when there is a suspected issue.</p> <p>Centralised monitoring and metrics gathered from the control plane have a 14-day retention period, after which data is automatically deleted.</p>"},{"location":"install-agent/","title":"Add your first agent to actuated","text":"<p>actuated is split into three parts:</p> <ol> <li>An Actuated Agent (agent) that you run on your own machines or VMs (server), which can launch a VM with a single-use GitHub Actions runner.</li> <li>A VM image launched by the agent, with all the preinstalled software found on a hosted GitHub Actions runner.</li> <li>Our own control plane that talks to GitHub on your behalf, and schedules builds across your fleet of agents.</li> </ol> <p>We look after 2 and 3 which means you just have to set up one or more agent to get started.</p> <p>Have you registered your organisation yet?</p> <p>Before you can add an agent, you or your GitHub organisation admin will need to install the: Actuated GitHub App.</p>"},{"location":"install-agent/#pick-your-actuated-servers","title":"Pick your Actuated Servers","text":"<p>Pick your Actuated Servers carefully using our guide: Pick a host for actuated</p>"},{"location":"install-agent/#review-the-end-user-license-agreement-eula","title":"Review the End User License Agreement (EULA)","text":"<p>Make sure you've read the Actuated EULA before registering your organisation with the actuated GitHub App, or starting the agent binary on one of your hosts.</p>"},{"location":"install-agent/#install-the-actuated-agent","title":"Install the Actuated Agent","text":"<ol> <li> <p>Download the Actuated Agent and installation script to the server</p> <p>Setting up an ARM64 agent? Wherever you see <code>agent</code> in a command, change it to: <code>agent-arm64</code>. So instead of <code>agent keygen</code> you'd run <code>agent-aarch64 keygen</code>.</p> <p>Install crane:</p> <pre><code>curl -sLS https://get.arkade.dev | sudo sh\narkade get crane\nsudo mv $HOME/.arkade/bin/crane /usr/local/bin/\n</code></pre> <p>Download the latest agent and install the binary to <code>/usr/local/bin/</code>:</p> <pre><code>rm -rf agent\nmkdir -p agent\ncrane export ghcr.io/openfaasltd/actuated-agent:latest | tar -xvf - -C ./agent\n\nsudo mv ./agent/agent* /usr/local/bin/\n</code></pre> <p>Run the setup.sh script which will install all the required dependencies like containerd, CNI and Firecracker.</p> <pre><code>cd agent\nsudo ./install.sh\n</code></pre> <p>Create a file to store your license. If you don't have it handy, check your email for your receipt.</p> <pre><code>mkdir -p ~/.actuated\n\n# Paste the contents, hit enter, then Control + D\n# Or edit the file with nano/vim\ncat &gt; $HOME/.actuated/LICENSE\n</code></pre> </li> <li> <p>Generate your enrollment file</p> <p>You can generate an enrollment file at <code>$HOME/.actuated/agent.yaml</code> by running:</p> <pre><code>agent enroll --url https://server1.example.com\n</code></pre> <p>The enrollment file contains:</p> <ul> <li>The hostname of the server</li> <li>The public key of the agent which we use to encrypt tokens sent to the agent to bootstrap runners to GitHub Actions</li> <li>A unique API token encrypted with our public key, which is used by the control plane to authenticate each message sent to the agent</li> </ul> </li> <li> <p>Expose the agent's endpoint using HTTPS</p> <p>The actuated control plane will only communicate with a HTTPS endpoint to ensure properly encryption is in place. An API token is used in addition with the TLS connection for all requests.</p> <p>In addition, any bootstrap tokens sent to the agent are further encrypted with the agent's public key.</p> <p>For hosts with public IPs, you will need to use the built-in TLS provisioning with Let's Encrypt. For hosts behind a firewall, NAT or in a private datacenter, you can use inlets to create a secure tunnel to the agent.</p> <p>We're considering other models for after the pilot, for instance GitHub's own API has the runner make an outbound connection and uses long-polling.</p> <p>See also: expose the agent with HTTPS</p> </li> <li> <p>Start the agent</p> <p>For an Intel/AMD Actuated Agent, create a <code>start.sh</code> file:</p> <pre><code>#!/bin/bash\n\necho Running Agent from: ./agent\nDOMAIN=agent1.example.com\n\nsudo -E agent up \\\n--image-ref=ghcr.io/openfaasltd/actuated-ubuntu20.04:x86-64-latest \\\n--kernel-ref=ghcr.io/openfaasltd/actuated-kernel-5.10.77:x86-64-latest \\\n--letsencrypt-domain $DOMAIN \\\n--letsencrypt-email webmaster@$DOMAIN\n</code></pre> <p>For an Actuated Agent behind an inlets tunnel:</p> <pre><code>#!/bin/bash\n\necho Running Agent from: ./agent\nsudo -E agent up \\\n--image-ref=ghcr.io/openfaasltd/actuated-ubuntu20.04:aarch64-latest \\\n--kernel-ref=ghcr.io/openfaasltd/actuated-kernel-5.10.77:aarch64-latest \\\n--listen-addr 127.0.0.1:\n</code></pre> <p>For ARM64 Actuated Servers, change the prefix of the image tags from <code>x86-64-</code> to <code>aarch64-</code></p> <p>You can also run the Actuated Agent software as a systemd unit file for automatic restarts and to start upon boot-up.</p> <p>An example of a systemd file you can use:</p> <p><pre><code>cat &lt;&lt;EOF &gt; actuated.service\n[Unit]\nDescription=Actuated Agent\n[Service]\nUser=root\nType=simple\nExecStart=/root/start.sh\nRestart=always\nRestartSec=5s\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> Double-check the <code>ExecStart</code> directive to make sure it includes the path to your <code>start.sh</code> file.</p> <p>The usual place to save the unit file is: <code>/etc/systemd/system/actuated.service</code>:</p> <pre><code>sudo cp actuated.service /etc/systemd/system/\n</code></pre> <p>After saving your service file, you can start the service for the first time.</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable --now actuated\n</code></pre> <p>Verify that it is running:</p> <pre><code>systemctl status actuated\n</code></pre> </li> <li> <p>Send us your agent's connection info</p> <p>Share the <code>$HOME/.actuated/agent.yaml</code> file with us so we can add your agent to the actuated control plane.</p> <p>We'll let you know once we've added your agent to actuated and then it's over to you to start running your builds.</p> </li> </ol>"},{"location":"install-agent/#next-steps","title":"Next steps","text":"<p>You can now start your first build and see it run on your actuated agent.</p> <p>Start a build on your agent</p> <p>See also: Troubleshooting your agent</p>"},{"location":"provision-server/","title":"Provision a Server","text":""},{"location":"provision-server/#provision-a-server-for-actuated","title":"Provision a Server for actuated","text":"<p>You'll need to provision or allocate a Server which is capable of virtualisation. This is because each of your builds will run in an isolated microVM, with its own networking, Kernel and filesystem.</p> <p>We regularly update our recommendations, which include bare-metal and cloud VMs which support nested virtualisation. These do not need to be overly expensive, if performance is not a concern, DigitalOcean VMs are available from tens of dollars per month, but a bare-metal server with a CPU that will run at a burst capacity of 4.4GHz, is around 100 USD / mo. If you have your own hardware, this can also be a really cost effective way of adopting actuated.</p>"},{"location":"provision-server/#intelamd-aka-x86_64","title":"Intel/AMD aka <code>x86_64</code>","text":"<p>There are a number of names and codenames for what we all know as \"regular PCs\" - with either an Intel or AMD CPU, the codenames are <code>x86_64</code> and <code>amd64</code> which are used interchangeably.</p> <ol> <li> <p>Bare-metal on-premises (cheap, convenient, high performance)</p> <p>Running bare-metal on-premises is a cost-effective and convenient way to re-use existing hardware investment.</p> <p>The machine could be racked in your server room, under your desk, or in a co-lo somewhere.</p> <p>Make sure you segment or isolate the agent into its own subnet, VLAN, DMZ, or VPC so that it cannot access the rest of your network. If you are thinking of running an actuated runner at home, we can share some iptables rules that worked well for our own testing.</p> </li> <li> <p>Bare-metal on the cloud (higher cost, convenient, high performance)</p> <p>Bare-metal doesn't have to mean managing hardware in your own physical datacenter. You can deploy machines by API, pay-as-you-go and get the highest performance available.</p> <p>Bear in mind that whilst the cost of bare-metal is higher than VMs, you will be able to pack more builds into them and get better throughput since actuated can schedule builds much more efficiently than GitHub's self-hosted runner.</p> <p>We have seen the best performance from hosts with high clock speeds like the recent generation of AMD processors, combined with local NVMe storage. Rotational drives and SATA SSDs are significantly slower. At the lower end of bare-metal providers, you'll pay 40-50 EUR / mo per host, moving up to 80-150 EUR / mo for NVMe and AMD processors, when you go up to enterprise-grade bare-metal with 10Gbit uplinks, you'll be more in the range of 500-1500 USD / mo.</p> <p>There are at least a dozen options for hosted bare-metal: Equinix Metal, Ionos, Hetzner, AWS, Cherry Servers, Alibaba Cloud, OVHcloud, fasthosts, Scaleway and Vultr, see a list here</p> <p>For x86_64 builds we recommend using Equinix Metal for the best price / performance ratio. They also have discounts for reserved instances on contract. The smallest instances available are the c3.small.x86 and c2.small.x86.</p> <p>We have done customer testing with Ionos with AMD CPUs and local NVMe, these are very quick and are good value.</p> <p>Having tested several of Scaleway's servers, we do not recommend their current generation of bare-metal due to I/O bottlenecks.</p> </li> <li> <p>Cloud Virtual Machines (VMs) that support nested virtualization (lowest cost, convenient, mid-level performance)</p> <p>This option may not have the raw speed and throughput of a dedicated, bare-metal host, but keeps costs low and is convenient for getting started.</p> <p>We know of at least three providers which have options for nested virtualisation: DigitalOcean, Google Compute Platform (GCP) (new customers get 300 USD free credits from GCP) support nested virtualisation on their Virtual Machines (VMs), and Azure.</p> </li> </ol>"},{"location":"provision-server/#arm64-aka-aarch64","title":"ARM64 aka <code>aarch64</code>","text":"<p>ARM CPUs are highly efficient when it comes to power consumption and pack in many more cores than the typical x86_64 CPU. This makes them ideal for running many builds in parallel. In typical testing, we've seen ARM builds running under emulation taking 35-45 minutes being reduced to 1-3 minutes total.</p> <ol> <li> <p>ARM for on-premises</p> <p>For on-premises ARM64 builds, we recommend the Mac Mini M1 (2020) with 16GB RAM and 512GB storage with Asahi Linux.</p> <p>The Raspberry Pi 4 also works, and in one instance was much faster than using emulation with a Hosted GitHub Runner.</p> <p>It's also possible to buy a 1 or 2U server from Ampere through one of their partners, with a capital expense up front.</p> </li> <li> <p>ARM on-demand, in the cloud</p> <p>For ARM64, Hetzner provides outstanding value in their RX-Line with 128GB / 256GB RAM coupled with NVMe and 80 cores for around 200 EUR / mo. These are Ampere Altra Servers. There is a minimum commitment of one month, and an initial setup cost per server.</p> <p>Following on from that, you have the a1.metal instance on AWS with 16 cores and 30GB / RAM for roughly 300 USD / mo. This is ideal if you already have an account with AWS and want to pay per minute.</p> <p>For a step up on specs, take a look at the c3.large.arm64 (Ampere Altra) from Equinix Metal comes with enterprise-grade networking and faster uplinks. These machines come in at around 2.5 USD / hour, but are packed out with many cores and other benefits.</p> </li> <li> <p>ARM VMs with nested virtualisation</p> <p>The current generations of ARM servers available from cloud providers do not support KVM, or nested virtualisation, which means you need to pick from the previous two options.</p> <p>There are ARM VMs available on Azure, GCP, and Oracle OCI. We have tested each and since they are based on the same generation of Ampere Altra hardware, we can confirm that they do not have KVM available and will not work for running actuated builds.</p> </li> </ol>"},{"location":"provision-server/#server-operating-system","title":"Server Operating System","text":"<p>The recommended Operating System for an Actuated Agent is: Ubuntu Server 22.04 or Ubuntu Server 20.04.</p> <p>Still not sure which option is right for your team? Get in touch with us on the Actuated Slack and we'll help you decide.</p>"},{"location":"provision-server/#next-steps","title":"Next steps","text":"<p>Now that you've created a Server or VM with the recommended Operating System, you'll need to install the actuated agent and get in touch with us, to register it.</p> <ul> <li>Install the Actuated Agent</li> </ul>"},{"location":"register/","title":"Register your GitHub Organisation","text":"<p>Would you like to try out Actuated for your team and GitHub Organisation?</p>"},{"location":"register/#what-youll-need","title":"What you'll need","text":"<ul> <li>A GitHub organisation (you can also create a test organisation to try actuated out)</li> <li>One or more public or private repositories hosted in the organisation</li> <li>Administrative access to add our GitHub App</li> <li>One or more bare-metal hosts or VMs that support nested virtualisation (if you need a recommendation, feel free to ask)</li> </ul>"},{"location":"register/#sign-up-for-the-pilot","title":"Sign-up for the pilot","text":"<p>Actuated is a managed service. We run a SaaS platform, integration with GitHub and VM images. All you need to do, is to install our agent on a number servers.</p> <p>What is a pilot?</p> <p>Actuated is already being used by commercial teams to run their pipelines, so why do we need a pilot? The pilot is a way for us to:</p> <ul> <li>build the right thing with customer feedback from day one</li> <li>land on the right pricing model</li> <li>find companies that are excited to work with us on solving this problem</li> </ul> <p>Sign-up for the pilot</p>"},{"location":"register/#install-the-github-app","title":"Install the GitHub App","text":"<p>Once accepted into the pilot program, an administrator from your GitHub organisation will need to install our GitHub App. GitHub Apps provide fine-grained access controls for third parties integrating with GitHub.</p> <p>Learn more in the FAQ. </p> <p>End User License Agreement (EULA)</p> <p>Make sure you've read the Actuated EULA before registering your organisation with the actuated GitHub App.</p> <ol> <li>Open the link that you received to install our GitHub App</li> <li>Click Install app</li> <li>Select the organisation you want to install the Actuated app to</li> <li> <p>Install the app on all repositories or select repositories</p> <p></p> </li> <li> <p>Once installed you will will see the permissions and other configuration options for the Actuated app on your selected account. To install the app on another account you can repeat these installation steps.</p> </li> </ol> <p>To manage or uninstall the Actuated app navigate to \"Settings\" for your repository or organisation and click on \"GitHub Apps\" in the left sidebar. Select the Actuated app from the list and click \"configure\".</p>"},{"location":"register/#next-steps","title":"Next steps","text":"<p>Now that you've installed the GitHub App, and picked a subscription plan:</p> <ul> <li>Provision a server</li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":"<p>Actuated is in a pilot phase, running builds for participating customers. The core experience is functioning and we are dogfooding it for actuated itself, OpenFaaS Pro and inlets.</p> <p>Our goal with the pilot is to prove that there's market fit for a solution like this, and if so, we'll invest more time in automation, user experience, agent autoscaling, dashboards and other polish.</p> <p>The technology being used is run at huge scale in production by AWS (on Lambda and Fargate) and GitHub (self-hosted runners use the same runner software and control plane).</p> <p>We believe actuated solves a real problem making CI fast, efficient, and multi-arch.</p> <p>If you'd like to try it out for your team, Register interest for the pilot.</p> <p>Shipped</p> <ul> <li> Firecracker MicroVM support for runners</li> <li> Secure builds for both public and private repos</li> <li> Fat VM image to match tooling installed by GitHub Actions</li> <li> KinD support for runner's Kernel</li> <li> K3s support for runner's Kernel</li> <li> ARM64 support, including Raspberry Pi 4B</li> <li> Efficient scheduling of jobs across fleet of agents</li> <li> Samples for K3s/KinD/Matrix builds and OpenFaaS functions</li> <li> Subscription plans delivered by Gumroad</li> <li> API for reviewing connected agents and queue depth</li> <li> Job event auditing for review via API</li> <li> Documentation site with detailed GitHub Actions examples</li> <li> Customer dashboard UI to show connected agents and build queue</li> <li> Official website actuated.dev</li> <li> Remote / automated update of agents via control plane</li> <li> Blog feature on actuated.dev with news, tutorials and updates from our team</li> <li> Performance testing for Ionos &amp; Scaleway for cost effective AMD bare-metal</li> <li> Daily build statistics on your dashboard</li> <li> Docker cache directly on the Actuated Hosts (servers) for much faster builds and avoiding rate-limiting</li> <li> Subscriptions: migration to LemonSqueezy for lower fees, and more payment options</li> <li> Dashboard - animation on all data pages for better feedback when refreshing data</li> </ul> <p>Coming soon:</p> <ul> <li> Detailed insights across your organisation on usage</li> <li> Support for private, self-hosted GitHub Enterprise Server (GHES) installations</li> </ul> <p>Under consideration:</p> <ul> <li> Actuated for GitLab</li> <li> Actuated for Jenkins</li> <li> GPU pass-through support for ML and AI workloads</li> <li> Automated agent installation and bootstrap</li> </ul> <p>Is there something else you need? If you're already a customer, contact us via the actuated Slack or Register for interest.</p>"},{"location":"test-build/","title":"Run a test build","text":"<p>Now that you've registered your GitHub organisation, created a server, and configured the agent, you're ready for a test build.</p> <p>We recommend you run the following build without changes to confirm that everything is working as expected. After that, you can modify an existing build and start using actuated for your team.</p> <p>The below steps should take less than 10 minutes.</p>"},{"location":"test-build/#create-a-repository-and-workflow","title":"Create a repository and workflow","text":"<p>This build will show you the specs, OS and Kernel name reported by the MicroVM.</p> <ol> <li> <p>Create a test repository and a GitHub Action</p> <p>Create <code>./.github/workflows/ci.yaml</code>:</p> <pre><code>name: CI\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n- main\nworkflow_dispatch:\n\njobs:\nspecs:\nname: specs\nruns-on: actuated\nsteps:\n- uses: actions/checkout@v1\n- name: Check specs\nrun: |\n./specs.sh\n</code></pre> <p>Note that the <code>runs-on:</code> field says <code>actuated</code> and not <code>ubuntu-latest</code>. This is how the actuated control plane knows to send this job to your agent.</p> <p>Then add <code>specs.sh</code> to the root of the repository:</p> <pre><code>#!/bin/bash\n\necho Information on main disk\ndf -h /\n\necho Memory info\nfree -h\n\necho Total CPUs:\necho CPUs: $(nproc)\n\necho CPU Model\ncat /proc/cpuinfo |grep \"model name\"\n\necho Kernel and OS info\nuname -a\n\nif ! [ -e /dev/kvm ]; then\necho \"/dev/kvm does not exist\"\nelse\necho \"/dev/kvm exists\"\nfi\n\necho OS\ncat /etc/os-release\n\necho Egress IP:\ncurl -s -L -S https://checkip.amazonaws.com\n</code></pre> </li> <li> <p>Hit commit, and watch the VM boot up.</p> <p>You'll be able to see the runners registered for your organisation on the Actuated Dashboard along with the build queue and stats for the current day's builds.</p> </li> <li> <p>If you're curious</p> <p>You can view the logs of the agent by logging into one of the Actuated Servers with SSH and running the following commands:</p> <pre><code>sudo journalctl -u actuated -f -o cat\n\n# Just today's logs:\nsudo journalctl -u actuated --since today -o cat\n</code></pre> <p>And each VM writes the logs from its console and the GitHub Actions Runner to <code>/var/log/actuated/</code>.</p> <pre><code>sudo cat /var/log/actuated/*\n</code></pre> </li> </ol> <p>Do you have any questions or comments? Feel free to reach out to us over Slack in the public channel for support.</p>"},{"location":"test-build/#enable-actuated-for-an-existing-repository","title":"Enable actuated for an existing repository","text":"<p>To add actuated to an existing repository, simply edit the workflow YAML file and change <code>runs-on:</code> to <code>runs-on: actuated</code>.</p> <p>If you want to go back to a hosted runner, edit the field back to <code>runs-on: ubuntu-latest</code> or whatever you used prior to that.</p>"},{"location":"test-build/#recommended-enable-a-docker-hub-mirror","title":"Recommended: Enable a Docker Hub mirror","text":"<p>We provide an add-on for setting up a cache/mirror of the Docker Hub. If you do not enable this, and use the Docker Hub in your builds, then you may run into the rate limits imposed by Docker Hub for anonymous users.</p> <p>See also: Set up a registry mirror</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#getting-support","title":"Getting support","text":"<p>All customers have access to a public Slack channel for support and collaboration.</p> <p>Enterprise customers may also have an upgraded SLA for support tickets via email and access to a private Slack channel.</p>"},{"location":"troubleshooting/#the-actuated-dashboard","title":"The Actuated Dashboard","text":"<p>The first port of call should be the Actuated Dashboard where you can check the status of your agents and see the current queue of jobs.</p> <p>For security reasons, an administrator for your GitHub Organisation will need to approve the Actuated Dashboard for access to your organisation before team members will be able to see any data. Send them the link for the dashboard, and have them specifically tick the checkbox for your organisation when logging in for the first time.</p> <p>If you missed this step, have them head over to their Applications Settings page, click \"Authorized OAuth Apps\" and then \"Actuated Dashboard\". On this page, under \"Organization access\" they can click \"Grant\" for each of your organisations registered for actuated.</p> <p></p> <p>How to \"Grant\" access to the Dashboard.</p> <p>Try a direct link here: Actuated Dashboard OAuth App</p>"},{"location":"troubleshooting/#a-job-is-stuck-as-queued","title":"A job is stuck as \"queued\"","text":"<p>By default, we reject jobs on public repositories, however we can feature flag this on for you. Just ask us.</p> <p>If you're using a private repo and the job is queued, let us know on Slack and we'll check the audit database to try and find out why.</p> <p>You can also check <code>/var/log/actuated/</code> for log files, <code>tail -n 20 /var/log/actuated/*.txt</code> should show you any errors that may have occurred on any of the VM boot-ups or runner registrations.</p> <p>From time to time, GitHub Actions does have an outage.</p>"},{"location":"troubleshooting/#you-pull-a-lot-of-large-images-from-the-docker-hub","title":"You pull a lot of large images from the Docker Hub","text":"<p>As much as we like to make our images as small as possible, sometimes we just have to pull down either large artifacts or many smaller ones. It just can't be helped.</p> <p>Since a MicroVM is a completely immutable environment, the pull needs to happen on each build, which is actually a good thing.</p> <p>The pull speed can be dramatically improved by using a registry mirror on each agent:</p> <ul> <li>Example: Set up a registry mirror</li> </ul>"},{"location":"troubleshooting/#you-are-running-into-rate-limits-when-using-container-images-from-the-docker-hub","title":"You are running into rate limits when using container images from the Docker Hub","text":"<p>The Docker Hub implements stringent rate limits of 100 pulls per 6 hours, and 200 pulls per 6 hours if you log in. Pro accounts get an increased limit of 5000 pulls per 6 hours.</p> <p>We've created simple instructions on how to set up a registry mirror to cache images on your Actuated Servers.</p> <ul> <li>Example: Set up a registry mirror</li> </ul>"},{"location":"troubleshooting/#you-need-to-rotate-the-authentication-token-used-for-your-agent","title":"You need to rotate the authentication token used for your agent","text":"<p>There should not be many reasons to rotate this token, however, if something's happened and it's been leaked or an employee has left the company, contact us via email for the update procedure.</p>"},{"location":"troubleshooting/#you-need-to-rotate-your-privatepublic-keypair","title":"You need to rotate your private/public keypair","text":"<p>Your private/public keypair is comparable to an SSH key, although it cannot be used to gain access to your agent via SSH.</p> <p>If you need to rotate it for some reason, please contact us by email as soon as you can.</p>"},{"location":"troubleshooting/#disk-space-is-running-out","title":"Disk space is running out","text":"<p>This can be observed by running <code>df -h</code> or <code>df -h /</code>.</p> <p>Over time, the various \"fat\" VM images that we ship to you may fill up the disk space on your machine.</p> <p>You can delete all images, including the current image with the following command.</p> <pre><code>sudo ctr -n mvm image ls -q | xargs sudo ctr -n mvm image rm\n</code></pre> <p>We do not recommend running this on a cron schedule since the maintenance command will cause your agent to download the latest fat VM image ~ 1GB\u00b1 again.</p> <p>An alternative for a cron schedule would need to exclude the current image being used:</p> <pre><code>CURRENT=\"ghcr.io/openfaasltd/actuated-ubuntu:20.0.4-2022-09-30-1357\"\nsudo ctr -n mvm image ls -q |grep -v $CURRENT | xargs sudo ctr -n mvm image rm\n</code></pre>"},{"location":"troubleshooting/#your-agent-has-been-offline-or-unavailable-for-a-significant-period-of-time","title":"Your agent has been offline or unavailable for a significant period of time","text":"<p>If your agent has been offline for a significant period of time, then our control plane will have disconnected it from its pool of available agents.</p> <p>Contact us via Slack to have it reinstated.</p>"},{"location":"troubleshooting/#the-devmapper-snapshotter-is-not-available-or-running","title":"The devmapper snapshotter is not available or running","text":"<p>The actuated agent uses the devmapper snapshotter for containerd, which emulates a thin-provisioned block device. Performance can be improved by attaching a dedicated disk or partition, but in our testing the devmapper works well enough for most workloads.</p> <p>The dmsetup.sh script must be run upon every fresh boot-up of the host. It enables Firecracker to use snapshots to save disk space when launching new VMs.</p> <p>If you see an error about the \"devmapper\" snapshot driver, then run the <code>dmsetup.sh</code> shell script then restart containerd:</p> <pre><code>./dmsetup.sh\nsudo systemctl daemon-reload\nsudo systemctl restart containerd\n</code></pre>"},{"location":"troubleshooting/#your-builds-are-slower-than-expected","title":"Your builds are slower than expected","text":"<ul> <li>Check free disk space (<code>df -h</code>)</li> <li>Check for unattended updates/upgrades (<code>ps -ef | grep unattended-upgrades</code>) and (<code>ps -ef | grep apt</code>)</li> </ul> <p>If you're using spinning disks, then consider switching to SSDs. If you're already using SSDs, consider using PCIe/NVMe SSDs.</p> <p>Finally, we do have another way to speed up microVMs by attaching another drive or partition to your host. Contact us for more information.</p>"},{"location":"examples/debug-ssh/","title":"Example: Debug a job with SSH","text":"<p>If your tier and subscription includes debugging with SSH, then you can follow these instructions to get a shell into your self-hosted runner.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/debug-ssh/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a secret for the repo or organisation for <code>SSH_GATEWAY_IP</code> using the IP address, or DNS address that you were provided with by your support team.</p> <p>Create a <code>.github/workflows/workflow.yaml</code> file</p> <pre><code>name: connect\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n\npermissions:\nid-token: write\ncontents: read\nactions: read\n\njobs:\nconnect:\nname: connect\nruns-on: actuated\nsteps:\n- name: Setup SSH server for Actor\nuses: alexellis/setup-sshd-actor@master\n- name: Connect to the actuated SSH gateway\nuses: alexellis/actuated-ssh-gateway-action@master\nwith:\ngatewayaddr: ${{ secrets.SSH_GATEWAY_IP }}\nsecure: true\n- name: Setup a blocking tmux session\nuses: alexellis/block-with-tmux-action@master\n</code></pre> <p>Next, trigger a build.</p> <p>Open <code>https://$SSH_GATEWAY_IP/list</code> in your browser and look for your session, you can log in using the SSH command outputted for you.</p> <p>Watch a demo:</p>"},{"location":"examples/docker/","title":"Example: Kubernetes with KinD","text":"<p>Docker CE is preinstalled in the actuated VM image, and will start upon boot-up.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/docker/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workspaces/build.yml</code> and commit it to the repository.</p> <p>Try running a container to ping Google for 3 times:</p> <pre><code>name: build\n\non: push\njobs:\nping-google:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: Run a ping to Google with Docker\nrun: |\ndocker run --rm -i alpine:latest ping -c 3 google.com\n</code></pre> <p>Build a container with Docker:</p> <pre><code>name: build\n\non: push\njobs:\nbuild-in-docker:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: Build inlets-connect using Docker\nrun: |\ngit clone --depth=1 https://github.com/alexellis/inlets-connect\ncd inlets-connect\ndocker build -t inlets-connect .\ndocker images\n</code></pre> <p>To run this on ARM64, just change the actuated label to <code>actuated-aarch64</code>.</p>"},{"location":"examples/github-actions-cache/","title":"Example: GitHub Actions cache","text":"<p>Jobs on Actuated runners start in a clean VM each time. This means dependencies need to be downloaded and build artifacts or caches rebuilt each time. Caching these files in the actions cache can improve workflow execution time.</p> <p>A lot of the setup actions for package managers have support for caching built-in. See: setup-node, setup-python, etc. They require minimal configuration and will create and restore dependency caches for you.</p> <p>If you have custom workflows that could benefit from caching the cache can be configured manually using the actions/cache.</p> <p>Using the actions cache is not limited to GitHub hosted runners but can be used with self-hosted runners. Workflows using the cache action can be converted to run on Actuated runners. You only need to change <code>runs-on: ubuntu-latest</code> to <code>runs-on: actuated</code>.</p>"},{"location":"examples/github-actions-cache/#use-the-github-actions-cache","title":"Use the GitHub Actions cache","text":"<p>In this short example we will build alexellis/registry-creds. This is a Kubernetes operator that can be used to replicate Kubernetes ImagePullSecrets to all namespaces.</p>"},{"location":"examples/github-actions-cache/#enable-caching-on-a-supported-action","title":"Enable caching on a supported action","text":"<p>Create a new file at: <code>.github/workspaces/build.yaml</code> and commit it to the repository.</p> <pre><code>name: build\n\non: push\n\njobs:\nbuild:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@v3\nwith:\nrepository: \"alexellis/registry-creds\"\n- name: Setup Golang\nuses: actions/setup-go@v3\nwith:\ngo-version: ~1.19\ncache: true\n- name: Build\nrun: |\nCGO_ENABLED=0 GO111MODULE=on \\\ngo build -ldflags \"-s -w -X main.Release=dev -X main.SHA=dev\" -o controller\n</code></pre> <p>To configure caching with the setup-go action you only need to set the <code>cache</code> input parameter to true.</p> <p>The cache is populated the first time this workflow runs. Running the workflow after this should be significantly faster because dependency files and build outputs are restored from the cache.</p>"},{"location":"examples/github-actions-cache/#manually-configure-caching","title":"Manually configure caching","text":"<p>If there is no setup action for your language that supports caching it can be configured manually.</p> <p>Create a new file at: <code>.github/workspaces/build.yaml</code> and commit it to the repository.</p> <pre><code>name: build\n\non: push\n\njobs:\nbuild:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@v3\nwith:\nrepository: \"alexellis/registry-creds\"\n- name: Setup Golang\nuses: actions/setup-go@v3\nwith:\ngo-version: ~1.19\ncache: true\n- name: Setup Golang caches\nuses: actions/cache@v3\nwith:\npath: |\n~/.cache/go-build\n~/go/pkg/mod\nkey: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}\nrestore-keys: |\n${{ runner.os }}-go-\n- name: Build\nrun: |\nCGO_ENABLED=0 GO111MODULE=on \\\ngo build -ldflags \"-s -w -X main.Release=dev -X main.SHA=dev\" -o controller\n</code></pre> <p>The setup <code>Setup Golang caches</code> uses the cache action to configure caching.</p> <p>The <code>path</code> parameter is used to set the paths on the runner to cache or restore. The <code>key</code> parameter sets the key used when saving the cache. A hash of the go.sum file is used as part of the cache key.</p>"},{"location":"examples/github-actions-cache/#further-reading","title":"Further reading","text":"<ul> <li>Checkout the list of <code>actions/cache</code> examples to configure caching for different languages and frameworks.</li> <li>See our blog: Make your builds run faster with Caching for GitHub Actions</li> </ul>"},{"location":"examples/k3s/","title":"Example: Kubernetes with k3s","text":"<p>You may need to access Kubernetes within your build. K3s is a for-production, lightweight distribution of Kubernetes that uses fewer resources than upstream. k3sup is a popular tool for installing k3s.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/k3s/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workspaces/build.yml</code> and commit it to the repository.</p> <p>Note that it's important to make sure Kubernetes is responsive before performing any commands like running a Pod or installing a helm chart.</p> <pre><code>name: k3sup-tester\n\non: push\njobs:\nk3sup-tester:\nruns-on: actuated\nsteps:\n- name: get arkade\nuses: alexellis/setup-arkade@v1\n- name: get k3sup and kubectl\nuses: alexellis/arkade-get@master\nwith:\nkubectl: latest\nk3sup: latest\n- name: Install K3s with k3sup\nrun: |\nmkdir -p $HOME/.kube/\nk3sup install --local --local-path $HOME/.kube/config\n- name: Wait until nodes ready\nrun: |\nk3sup ready --quiet --kubeconfig $HOME/.kube/config --context default\n- name: Wait until CoreDNS is ready\nrun: |\nkubectl rollout status deploy/coredns -n kube-system --timeout=300s\n- name: Explore nodes\nrun: kubectl get nodes -o wide\n- name: Explore pods\nrun: kubectl get pod -A -o wide\n</code></pre> <p>To run this on ARM64, just change the actuated label to <code>actuated-aarch64</code>.</p>"},{"location":"examples/kernel/","title":"Example: Test that compute time by compiling a Kernel","text":"<p>Use this sample to test the raw compute speed of your hosts by building a Kernel.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/kernel/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workspaces/build.yml</code> and commit it to the repository.</p> <pre><code>name: microvm-kernel\n\non: push\njobs:\nmicrovm-kernel:\nruns-on: actuated\nsteps:\n- name: free RAM\nrun: free -h\n- name: List CPUs\nrun: nproc\n- name: get build toolchain\nrun: |\nsudo apt update -qy\nsudo apt-get install -qy \\\ngit \\\nbuild-essential \\\nkernel-package \\\nfakeroot \\\nlibncurses5-dev \\\nlibssl-dev \\\nccache \\\nbison \\\nflex \\\nlibelf-dev \\\ndwarves\n- name: clone linux\nrun: |\ntime git clone https://github.com/torvalds/linux.git linux.git --depth=1 --branch v5.10\ncd linux.git\ncurl -o .config -s -f https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/resources/guest_configs/microvm-kernel-x86_64-5.10.config\necho \"# CONFIG_KASAN is not set\" &gt;&gt; .config\n- name: make config\nrun: |\ncd linux.git \nmake oldconfig\n- name: Make vmlinux\nrun: |\ncd linux.git\ntime make vmlinux -j$(nproc)\ndu -h ./vmlinux\n</code></pre> <p>When you have a build time, why not change <code>runs-on: actuated</code> to <code>runs-on: ubuntu-latest</code> to compare it to a hosted runner from GitHub?</p> <p>Here's our test, where our own machine built the Kernel 4x faster than a hosted runner:</p> <p></p>"},{"location":"examples/kind/","title":"Example: Kubernetes with KinD","text":"<p>You may need to access Kubernetes within your build. KinD is a popular option, and easy to run in an action.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/kind/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workspaces/build.yml</code> and commit it to the repository.</p> <p>Note that it's important to make sure Kubernetes is responsive before performing any commands like running a Pod or installing a helm chart.</p> <pre><code>name: build\n\non: push\njobs:\nstart-kind:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: get arkade\nuses: alexellis/setup-arkade@v1\n- name: get kubectl and kubectl\nuses: alexellis/arkade-get@master\nwith:\nkubectl: latest\nkind: latest\n- name: Create a KinD cluster\nrun: |\nmkdir -p $HOME/.kube/\nkind create cluster --wait 300s\n- name: Wait until CoreDNS is ready\nrun: |\nkubectl rollout status deploy/coredns -n kube-system --timeout=300s\n- name: Explore nodes\nrun: kubectl get nodes -o wide\n- name: Explore pods\nrun: kubectl get pod -A -o wide\n- name: Show kubelet logs\nrun: docker exec kind-control-plane journalctl -u kubelet\n</code></pre> <p>To run this on ARM64, just change the actuated label to <code>actuated-aarch64</code>.</p>"},{"location":"examples/kvm-guest/","title":"Example: Run a KVM guest","text":"<p>It is possible to launch a Virtual Machine (VM) within a GitHub Action. Support for virtualization is not enabled by default for Actuated. The Agent has to be configured to use a custom kernel.</p> <p>There are some prerequisites to enable KVM support:</p> <ul> <li><code>aarch64</code> runners are not supported at the moment.</li> <li>A bare-metal host for the Agent is required.</li> </ul>"},{"location":"examples/kvm-guest/#configure-the-agent","title":"Configure the Agent","text":"<ol> <li> <p>Make sure nested virtualization is enabled on the Agent host.</p> </li> <li> <p>Modify the <code>start.sh</code> file for your Actuated Agent and add the <code>kvm</code> suffix to the kernel-ref tag:</p> <pre><code>sudo -E agent up \\\n   --image-ref=ghcr.io/openfaasltd/actuated-ubuntu20.04:x86-64-latest \\\n-   --kernel-ref=ghcr.io/openfaasltd/actuated-kernel-5.10.77:x86-64-latest \\\n+   --kernel-ref=ghcr.io/openfaasltd/actuated-kernel-5.10.77:x86-64-latest-kvm \\\n   --listen-addr 127.0.0.1:\n</code></pre> </li> <li> <p>Restart the Agent to use the new kernel.</p> <pre><code>sudo systemctl restart actuated\n</code></pre> </li> <li> <p>Run a test build to verify KVM support is enabled in the runner. The specs script from the test build will report whether <code>/dev/kvm</code> is available.</p> </li> </ol>"},{"location":"examples/kvm-guest/#run-a-firecracker-microvm","title":"Run a Firecracker microVM","text":"<p>This example is an adaptation of the Firecracker quickstart guide that we run from within a GitHub Actions workflow.</p> <p>The workflow instals Firecracker, configures and boots a guest VM and then waits 20 seconds before shutting down the VM and exiting the workflow.</p> <ol> <li> <p>Create a new repository and add a workflow file.</p> <p>The workflow file: <code>./.github/workflows/vm-run.yaml</code>:</p> <pre><code>name: vm-run\n\non: push\njobs:\nvm-run:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: Install arkade\nuses: alexellis/setup-arkade@v2\n- name: Install firecracker\nrun: sudo arkade system install firecracker\n- name: Run microVM\nrun: sudo ./run-vm.sh\n</code></pre> </li> <li> <p>Add the <code>run-vm.sh</code> script to the root of the repository.</p> <p>Running the script will:</p> <ul> <li>Get the kernel and rootfs for the microVM</li> <li>Start fireckracker and configure the guest kernel and rootfs</li> <li>Start the guest machine</li> <li>Wait for 20 seconds and kill the firecracker process so workflow finishes.</li> </ul> <p>The <code>run-vm.sh</code> script:</p> <pre><code>#!/bin/bash\n\n# Clone the example repo\ngit clone https://github.com/skatolo/nested-firecracker.git\n\n# Run the VM script\n./nested-firecracker/run-vm.sh </code></pre> </li> <li> <p>Hit commit and check the run logs of the workflow. You should find the login prompt of the running microVM in the logs.</p> </li> </ol> <p>The full example is available on GitHub</p> <p>For more examples and use-cases see:</p> <ul> <li>How to run a KVM guest in your GitHub Actions</li> </ul>"},{"location":"examples/matrix-k8s/","title":"Example: Regression test against various Kubernetes versions","text":"<p>This example launches multiple Kubernetes clusters in parallel for regression and end to end testing.</p> <p>In the example, We're testing the CRD for the inlets-operator on versions v1.16 through to v1.25. You could also switch out k3s for KinD, if you prefer.</p> <p>See also: Actuated with KinD</p> <p></p> <p>Launching 10 Kubernetes clusters in parallel across your fleet of Actuated Servers.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/matrix-k8s/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workspaces/build.yml</code> and commit it to the repository.</p> <p>Customise both the array \"k3s\" with the versions you need to test and replace the step \"Test crds\" with whatever you need to install such as helm charts.</p> <pre><code>name: k3s-test-matrix\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n- main\n\njobs:\nkubernetes:\nname: k3s-test-${{ matrix.k3s }}\nruns-on: actuated\nstrategy:\nmatrix:\nk3s: [v1.16, v1.17, v1.18, v1.19, v1.20, v1.21, v1.22, v1.23, v1.24, v1.25]\n\nsteps:\n- uses: actions/checkout@v1\n- uses: alexellis/setup-arkade@v2\n- uses: alexellis/arkade-get@master\nwith:\nkubectl: latest\nk3sup: latest\n\n- name: Create Kubernetes ${{ matrix.k3s }} cluster\nrun: |\nmkdir -p $HOME/.kube/\nk3sup install \\\n--local \\\n--k3s-channel ${{ matrix.k3s }} \\\n--local-path $HOME/.kube/config \\\n--merge \\\n--context default\ncat $HOME/.kube/config\n\nk3sup ready --context default\nkubectl config use-context default\n\n# Just an extra test on top.\necho \"Waiting for nodes to be ready ...\"\nkubectl wait --for=condition=Ready nodes --all --timeout=5m\nkubectl get nodes -o wide\n\n- name: Test crds\nrun: |\necho \"Applying CRD\"\nkubectl apply -f https://raw.githubusercontent.com/inlets/inlets-operator/master/artifacts/crds/inlets.inlets.dev_tunnels.yaml\n</code></pre> <p>The matrix will cause a new VM to be launched for each item in the \"k3s\" array.</p>"},{"location":"examples/matrix/","title":"Example: matrix-build - run a VM per each job in a matrix","text":"<p>Use this sample to test launching multiple VMs in parallel.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/matrix/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workspaces/build.yml</code> and commit it to the repository.</p> <pre><code>name: CI\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n- main\n\njobs:\narkade-e2e:\nname: arkade-e2e\nruns-on: actuated\nstrategy:\nmatrix:\napps: [run-job,k3sup,arkade,kubectl,faas-cli]\nsteps:\n- name: Get arkade\nrun: |\ncurl -sLS https://get.arkade.dev | sudo sh\n- name: Download app\nrun: |\necho ${{ matrix.apps }}\narkade get ${{ matrix.apps }}\nfile /home/runner/.arkade/bin/${{ matrix.apps }}\n</code></pre> <p>The matrix will cause a new VM to be launched for each item in the \"apps\" array.</p>"},{"location":"examples/multiarch-buildx/","title":"Example: Multi-arch with buildx","text":"<p>A multi-arch or multi-platform container is effectively where you build the same container image for multiple different Operating Systems or CPU architectures, and link them together under a single name.</p> <p>So you may publish an image named: <code>ghcr.io/inlets-operator/latest</code>, but when this image is fetched by a user, a manifest file is downloaded, which directs the user to the appropriate image for their architecture.</p> <p>If you'd like to see what these look like, run the following with arkade:</p> <pre><code>arkade get crane\n\ncrane manifest ghcr.io/inlets/inlets-operator:latest\n</code></pre> <p>You'll see a manifests array, with a platform section for each image:</p> <pre><code>{\n\"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\",\n\"manifests\": [\n{\n\"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n\"digest\": \"sha256:bae8025e080d05f1db0e337daae54016ada179152e44613bf3f8c4243ad939df\",\n\"platform\": {\n\"architecture\": \"amd64\",\n\"os\": \"linux\"\n}\n},\n{\n\"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n\"digest\": \"sha256:3ddc045e2655f06653fc36ac88d1d85e0f077c111a3d1abf01d05e6bbc79c89f\",\n\"platform\": {\n\"architecture\": \"arm64\",\n\"os\": \"linux\"\n}\n}\n]\n}\n</code></pre>"},{"location":"examples/multiarch-buildx/#try-an-example","title":"Try an example","text":"<p>This example is taken from the Open Source inlets-operator.</p> <p>It builds a container image containing a Go binary and uses a Dockerfile in the root of the repository. All of the images and corresponding manifest are published to GitHub's Container Registry (GHCR). The action itself is able to authenticate to GHCR using a built-in, short-lived token. This is dependent on the \"permissions\" section and \"packages: write\" being set.</p> <p>View publish.yaml, adapted for actuated:</p> <pre><code>name: publish\n\non:\n push:\n    tags:\n      - '*'\n\njobs:\n publish:\n+    permissions:\n+      packages: write\n\n-   runs-on: ubuntu-latest\n+   runs-on: actuated\n   steps:\n      - uses: actions/checkout@master\n        with:\n          fetch-depth: 1\n\n+     - name: Setup mirror\n+       uses: self-actuated/hub-mirror@master\n     - name: Get TAG\n        id: get_tag\n        run: echo TAG=${GITHUB_REF#refs/tags/} &gt;&gt; $GITHUB_ENV\n      - name: Get Repo Owner\n        id: get_repo_owner\n        run: echo \"REPO_OWNER=$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]')\" &gt; $GITHUB_ENV\n\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v2\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n      - name: Login to container Registry\n        uses: docker/login-action@v2\n        with:\n          username: ${{ github.repository_owner }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n          registry: ghcr.io\n\n      - name: Release build\n        id: release_build\n        uses: docker/build-push-action@v3\n        with:\n          outputs: \"type=registry,push=true\"\n          platforms: linux/amd64,linux/arm/v6,linux/arm64\n          build-args: |\n            Version=${{  env.TAG }}\n            GitCommit=${{ github.sha }}\n          tags: |\n            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}\n            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ env.TAG }}\n            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:latest\n</code></pre> <p>You'll see that we added a <code>Setup mirror</code> step, this explained in the Registry Mirror example</p> <p>The <code>docker/setup-qemu-action@v2</code> step is responsible for setting up QEMU, which is used to emulate the different CPU architectures.</p> <p>The <code>docker/build-push-action@v3</code> step is responsible for passing in a number of platform combinations such as: <code>linux/amd64</code> for cloud, <code>linux/arm64</code> for Arm servers and <code>linux/arm/v6</code> for Raspberry Pi.</p> <p>Within the Dockerfile, we needed to make a couple of changes.</p> <p>You can pick to run the step in either the BUILDPLATFORM or TARGETPLATFORM. The BUILDPLATFORM is the native architecture and platform of the machine performing the build, this is usually amd64. The TARGETPLATFORM is important for the final step of the build, and will be injected based upon one each of the platforms you have specified in the step.</p> <pre><code>- FROM golang:1.18 as builder\n+ FROM --platform=${BUILDPLATFORM:-linux/amd64} golang:1.18 as builder\n</code></pre> <p>For Go specifically, we also updated the <code>go build</code> command to tell Go to use cross-compilation based upon the TARGETOS and TARGETARCH environment variables, which are populated by Docker.</p> <pre><code>GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o inlets-operator\n</code></pre> <p>Learn more in the Docker Documentation: Multi-platform images</p>"},{"location":"examples/multiarch-buildx/#is-it-slow-to-build-for-arm","title":"Is it slow to build for Arm?","text":"<p>Using QEMU can be slow at times, especially when building an image for Arm using a hosted GitHub Runner.</p> <p>We found that we could increase an Open Source project's build time by 22x - from ~ 36 minutes to 1 minute 26 seconds.</p> <p>See also How to make GitHub Actions 22x faster with bare-metal Arm</p> <p>To build a separate image for Arm on an Arm runner, and one for amd64, you could use a matrix build.</p>"},{"location":"examples/multiarch-buildx/#need-a-hand-with-github-actions","title":"Need a hand with GitHub Actions?","text":"<p>Check your plan to see if access to Slack is included, if so, you can contact us on Slack for help and guidance.</p>"},{"location":"examples/openfaas-helm/","title":"Example: Publish an OpenFaaS function","text":"<p>This example will create a Kubernetes cluster using KinD, deploy OpenFaaS using Helm, deploy a function, then invoke the function. There are some additional checks for readiness for Kubernetes and the OpenFaaS gateway.</p> <p>You can adapt this example for any other Helm charts you may have for E2E testing.</p> <p>We also recommend considering arkade for installing CLIs and common Helm charts for testing.</p> <p>Docker CE is preinstalled in the actuated VM image, and will start upon boot-up.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code></li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/openfaas-helm/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new GitHub repository in your organisation.</p> <p>Add: <code>.github/workflows/e2e.yaml</code></p> <pre><code>name: e2e\n\non:\npush:\nbranches:\n- '*'\npull_request:\nbranches:\n- '*'\n\npermissions:\nactions: read\ncontents: read\n\njobs:\ne2e:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: get arkade\nuses: alexellis/setup-arkade@v1\n- name: get kubectl and kubectl\nuses: alexellis/arkade-get@master\nwith:\nkubectl: latest\nkind: latest\nfaas-cli: latest\n- name: Install Kubernetes KinD\nrun: |\nmkdir -p $HOME/.kube/\nkind create cluster --wait 300s\n- name: Add Helm chart, update repos and apply namespaces\nrun: |\nkubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml\nhelm repo add openfaas https://openfaas.github.io/faas-netes/\nhelm repo update\n- name: Install the Community Edition (CE)\nrun: |\nhelm repo update \\\n&amp;&amp; helm upgrade openfaas --install openfaas/openfaas \\\n--namespace openfaas  \\\n--set functionNamespace=openfaas-fn \\\n--set generateBasicAuth=true\n- name: Wait until OpenFaaS is ready\nrun: |\nkubectl rollout status -n openfaas deploy/prometheus --timeout 5m\nkubectl rollout status -n openfaas deploy/gateway --timeout 5m\n- name: Port forward the gateway\nrun: |\nkubectl port-forward -n openfaas svc/gateway 8080:8080 &amp;\n\nattempts=0\nmax=10\n\nuntil $(curl --output /dev/null --silent --fail http://127.0.0.1:8080/healthz ); do\nif [ ${attempts} -eq ${max} ]; then\necho \"Max attempts reached $max waiting for gateway's health endpoint\"\nexit 1\nfi\n\nprintf '.'\nattempts=$(($attempts+1))\nsleep 1\ndone\n- name: Login to OpenFaaS gateway and deploy a function\nrun: |\nPASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath=\"{.data.basic-auth-password}\" | base64 --decode; echo)\necho -n $PASSWORD | faas-cli login --username admin --password-stdin \n\nfaas-cli store deploy env\n\nfaas-cli invoke env &lt;&lt;&lt; \"\"\n\ncurl -s -f -i http://127.0.0.1:8080/function/env\n\nfaas-cli invoke --async env &lt;&lt;&lt; \"\"\n\nkubectl logs -n openfaas deploy/queue-worker\n\nfaas-cli describe env\n</code></pre> <p>If you'd like to deploy the function, check out a more comprehensive example of how to log in and deploy in Serverless For Everyone Else</p>"},{"location":"examples/openfaas-publish/","title":"Example: Publish an OpenFaaS function","text":"<p>This example will publish an OpenFaaS function to GitHub's Container Registry (GHCR).</p> <ul> <li>The example uses Docker's buildx and QEMU for a multi-arch build</li> <li>Dynamic variables to inject the SHA and OWNER name from the repo</li> <li>Uses the token that GitHub assigns to the action to publish the containers.</li> </ul> <p>You can also run this example on GitHub's own hosted runners.</p> <p>Docker CE is preinstalled in the actuated VM image, and will start upon boot-up.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/openfaas-publish/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>For alexellis' repository called alexellis/autoscaling-functions, then check out the <code>.github/workspaces/publish.yml</code> file:</p> <ul> <li>The \"Setup QEMU\" and \"Set up Docker Buildx\" steps configure the builder to produce a multi-arch image.</li> <li>The \"OWNER\" variable means this action can be run on any organisation without having to hard-code a username for GHCR.</li> <li>Only the bcrypt function is being built with the <code>--filter</code> command added, remove it to build all functions in the stack.yml.</li> <li><code>--platforms linux/amd64,linux/arm64,linux/arm/v7</code> will build for regular Intel/AMD machines, 64-bit ARM and 32-bit ARM i.e. Raspberry Pi, most users can reduce this list to just \"linux/amd64\" for a speed improvement</li> </ul> <p>Make sure you edit <code>runs-on:</code> and set it to <code>runs-on: actuated</code></p> <pre><code>name: publish\n\non:\npush:\nbranches:\n- '*'\npull_request:\nbranches:\n- '*'\n\npermissions:\nactions: read\nchecks: write\ncontents: read\npackages: write\n\njobs:\npublish:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: Get faas-cli\nrun: curl -sLSf https://cli.openfaas.com | sudo sh\n- name: Pull custom templates from stack.yml\nrun: faas-cli template pull stack\n- name: Set up QEMU\nuses: docker/setup-qemu-action@v1\n- name: Set up Docker Buildx\nuses: docker/setup-buildx-action@v1\n- name: Get TAG\nid: get_tag\nrun: echo ::set-output name=TAG::latest-dev\n- name: Get Repo Owner\nid: get_repo_owner\nrun: &gt;\necho ::set-output name=repo_owner::$(echo ${{ github.repository_owner }} |\ntr '[:upper:]' '[:lower:]')\n- name: Docker Login\nrun: &gt; echo ${{secrets.GITHUB_TOKEN}} | \ndocker login ghcr.io --username \n${{ steps.get_repo_owner.outputs.repo_owner }} \n--password-stdin\n- name: Publish functions\nrun: &gt;\nOWNER=\"${{ steps.get_repo_owner.outputs.repo_owner }}\" \nTAG=\"latest\"\nfaas-cli publish\n--extra-tag ${{ github.sha }}\n--build-arg GO111MODULE=on\n--platforms linux/amd64,linux/arm64,linux/arm/v7\n--filter bcrypt\n</code></pre> <p>If you'd like to deploy the function, check out a more comprehensive example of how to log in and deploy in Serverless For Everyone Else</p>"},{"location":"examples/registry-mirror/","title":"Example: Set up a registry mirror","text":"<p>Use-cases:</p> <ul> <li>Increase speed of pulls and builds by caching images on Actuated Servers</li> <li>Reduce failed builds due to rate-limiting</li> </ul> <p>If you use Docker in your self-hosted builds, there is a chance that you'll run into the rather conservative rate-limits.</p> <p>The Docker Hub allows for 100 image pulls within a 6 hour period, but this can be extended to 200 by logging in, or to 5000 by paying for a Pro license.</p> <p>A registry mirror / pull-through cache running on an actuated agent is significantly faster than pulling from a remote server.</p> <p>We will create a mirror that:</p> <ul> <li>Has no authentication, to keep the changes to your build to a minimum</li> <li>Cannot support PUSH / PUT / DELETE events</li> <li>Only has access to pull images from the Docker Hub</li> <li>Is not exposed to the Internet, but only to Actuated VMs</li> <li>When unavailable for any reason, the build continues without error</li> <li>Works on both Intel/AMD and ARM64 hosts</li> </ul> <p>This tutorial shows you how to set up what was previously known as \"Docker's Open Source Registry\" and is now a CNCF project called distribution.</p>"},{"location":"examples/registry-mirror/#create-a-docker-hub-access-token","title":"Create a Docker Hub Access token","text":"<p>Create a Docker Hub Access token with \"Public repos only\" scope, and save it as <code>~/hub.txt</code> on the Actuated Server.</p> <p></p> <p>Settings for an authorization token, with read-only permissions to public repositories</p>"},{"location":"examples/registry-mirror/#set-up-the-registry-on-an-actuated-agent","title":"Set up the registry on an actuated agent","text":"<pre><code>curl -sLS https://get.arkade.dev | sudo sh\n\nsudo arkade system install registry\n\nsudo mkdir -p /etc/registry\nsudo mkdir -p /var/lib/registry\n</code></pre> <p>Create a config file to make the registry only available on the Linux bridge for Actuated VMs:</p> <pre><code>export TOKEN=$(cat ~/hub.txt)\nexport USERNAME=\"\"\n\ncat &gt;&gt; /tmp/registry.yml &lt;&lt;EOF\nversion: 0.1\nlog:\n  accesslog:\n    disabled: true\n  level: warn\n  formatter: text\n\nstorage:\n  filesystem:\n    rootdirectory: /var/lib/registry\n\nproxy:\n  remoteurl: https://registry-1.docker.io\n  username: $USERNAME\n\n  # A Docker Hub Personal Access token created with \"Public repos only\" scope\n  password: $TOKEN\n\nhttp:\n  addr: 192.168.128.1:5000\n  relativeurls: false\n  draintimeout: 60s\nEOF\n\nsudo mv /tmp/registry.yml /etc/registry/config.yml\n</code></pre> <p>Install and start the registry with a systemd unit file:</p> <pre><code>cat &gt;&gt; /tmp/registry.service &lt;&lt;EOF\n[Unit]\nDescription=Registry\nAfter=network.target\n\n[Service]\nType=simple\nRestart=always\nRestartSec=5s\nExecStart=/usr/local/bin/registry serve /etc/registry/config.yml\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n\nsudo mv /tmp/registry.service /etc/systemd/system/registry.service\nsudo systemctl daemon-reload\nsudo systemctl enable registry --now\n</code></pre> <p>Check the status with:</p> <pre><code>sudo journalctl -u registry\n</code></pre>"},{"location":"examples/registry-mirror/#use-the-registry-within-a-workflow","title":"Use the registry within a workflow","text":"<p>Create a new registry in your organisation, along with a: <code>.github/workspaces/build.yml</code> file and commit it to the repository.</p> <pre><code>name: CI\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n- main\n\njobs:\nbuild:\nruns-on: [actuated]\nsteps:\n\n- name: Setup mirror\nuses: self-actuated/hub-mirror@master\n\n- name: Checkout\nuses: actions/checkout@v2\n\n- name: Pull image using cache\nrun: |\ndocker pull alpine:latest\n</code></pre>"},{"location":"examples/registry-mirror/#checking-if-it-worked","title":"Checking if it worked","text":"<p>You'll see the build run, and cached artifacts appearing in: <code>/var/lib/registry/</code>.</p> <pre><code>find /var/lib/registry/ -name \"alpine\"\n\n/var/lib/registry/docker/registry/v2/repositories/library/alpine\n</code></pre> <p>You can also use the registry's API to query which images are available:</p> <pre><code>curl -i http://192.168.128.1:5000/v2/_catalog\n\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nDocker-Distribution-Api-Version: registry/2.0\nDate: Wed, 16 Nov 2022 09:41:18 GMT\nContent-Length: 52\n\n{\"repositories\":[\"library/alpine\",\"moby/buildkit\"]}\n</code></pre> <p>You can check the status of the mirror at any time with:</p> <pre><code>sudo journalctl -u registry --since today\n</code></pre>"},{"location":"examples/registry-mirror/#further-reading","title":"Further reading","text":"<ul> <li>Docker: Configuration for the registry</li> <li>GitHub: View the project on GitHub</li> </ul>"},{"location":"examples/system-info/","title":"Example: Get system information about your microVM","text":"<p>This sample reveals system information about your runner.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/system-info/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a specs.sh file:</p> <pre><code>#!/bin/bash\n\nhostname\n\nwhoami\n\necho Information on main disk\ndf -h /\n\necho Memory info\nfree -h\n\necho Total CPUs:\necho CPUs: nproc\n\necho CPU Model\ncat /proc/cpuinfo |grep \"model name\"\n\necho Kernel and OS info\nuname -a\n\ncat /etc/os-release\n\necho PATH defined as:\necho $PATH\n\necho Egress IP defined as:\ncurl -sLS https://checkip.amazonaws.com\n</code></pre> <p>Create a new file at: <code>.github/workspaces/build.yml</code> and commit it to the repository.</p> <pre><code>name: CI\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n\njobs:\nspecs:\nname: specs\nruns-on: actuated\nsteps:\n- uses: actions/checkout@v1\n- name: Check specs\nrun: |\n./specs.sh\n</code></pre> <p>Note how the hostname changes every time the job is run.</p>"}]}